{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "006a735f-df14-40fa-b507-e057eb5d9462",
   "metadata": {},
   "source": [
    "# Chunking.ipynb - Text Chunking Techniques for RAG Applications\n",
    "## Overview\n",
    "This Jupyter notebook demonstrates various text chunking strategies for Retrieval-Augmented Generation (RAG) applications. It explores different methods to split large documents into smaller, manageable chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f0206d-a652-4d08-a12a-4e4c0f09be4a",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The notebook uses Amazon shareholder letters from 2019-2022 as sample documents:\n",
    "\n",
    "- AMZN-2022-Shareholder-Letter.pdf\n",
    "- AMZN-2021-Shareholder-Letter.pdf\n",
    "- AMZN-2020-Shareholder-Letter.pdf\n",
    "- AMZN-2019-Shareholder-Letter.pdf\n",
    "\n",
    "These documents are automatically downloaded and stored in a ./data/ directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9f943c-2f24-4750-bb29-feeec51f4733",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./data\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "from urllib.request import urlretrieve\n",
    "urls = [\n",
    "    \"https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/2022-Shareholder-Letter.pdf\",\n",
    "    \"https://s2.q4cdn.com/299287126/files/doc_financials/2022/ar/2021-Shareholder-Letter.pdf\",\n",
    "    \"https://s2.q4cdn.com/299287126/files/doc_financials/2021/ar/Amazon-2020-Shareholder-Letter-and-1997-Shareholder-Letter.pdf\",\n",
    "    \"https://s2.q4cdn.com/299287126/files/doc_financials/2020/ar/2019-Shareholder-Letter.pdf\",\n",
    "]\n",
    "\n",
    "filenames = [\n",
    "    \"AMZN-2022-Shareholder-Letter.pdf\",\n",
    "    \"AMZN-2021-Shareholder-Letter.pdf\",\n",
    "    \"AMZN-2020-Shareholder-Letter.pdf\",\n",
    "    \"AMZN-2019-Shareholder-Letter.pdf\",\n",
    "]\n",
    "\n",
    "data_root = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2708ba9-4e90-4eaa-bc95-24a1570cfd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, url in enumerate(urls):\n",
    "    file_path = data_root + filenames[idx]\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2418178-177d-4a7c-8736-ecd8ab8a8e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "data_root = \"./data/\"\n",
    "folder_path = data_root\n",
    "documents = []\n",
    "\n",
    "# Loop through all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    # Load the PDF data\n",
    "    data = loader.load()\n",
    "    # Add the loaded data to the documents list\n",
    "    documents.extend(data)\n",
    "\n",
    "# Print the text of the first page of the first document\n",
    "if documents:\n",
    "    print(documents[0].page_content)\n",
    "else:\n",
    "    print(\"No PDF files found in the folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e132bc8-742d-497c-ac14-65f3428ac5dd",
   "metadata": {},
   "source": [
    "# Overlap chunking\n",
    "Use Case: Simple, fixed-size chunks with minimal overlap for context preservation\n",
    "- chunk_size: the maximum length (in characters) of each chunk or segment that the text will be split into.\n",
    "\n",
    "- chunk_overlap: the number of characters that should overlap between consecutive chunks. This overlap can help provide context to the subsequent chunks, especially when dealing with tasks that require understanding the surrounding context.\n",
    "\n",
    "- separator: a string that specifies the separators used to split the text into chunks. By default, it is set to \"\\n\\n\", which means that the splitter will split the text at occurrences of two consecutive newline characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562c31e4-f22e-4999-93d4-ec71358878a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab8386c-44c1-402b-89f8-6a6cc47c4060",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=10, separator=\"\")\n",
    "splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269ddc08-4a6b-4aaf-b442-f3a26fc5ac8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8504e6-f480-4f14-b892-00a68d3c85a6",
   "metadata": {},
   "source": [
    "# Recursive Character Splitting\n",
    "Use Case: More intelligent splitting that respects document structure (paragraphs → sentences → words)\n",
    "\n",
    "The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fb9ec1-7a5f-48d0-a9e5-f3891d98fb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "rec_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100, chunk_overlap=10, separators=[\"\\n\"]\n",
    ")\n",
    "rec_text_splits = rec_text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17abb2c2-fa8b-400b-a22a-5cae89c5eb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_text_splits[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ad3520-1213-47e9-90f0-d104597af95c",
   "metadata": {},
   "source": [
    "# Semantic Chunking\n",
    "Use Case: Content-aware chunking that maintains topical coherence\n",
    "\n",
    "Features:\n",
    "- Splits based on semantic similarity between sentences\n",
    "- Uses embedding vectors to determine natural break points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9c7402-e208-44c9-9579-04b70bb1b75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aacf098-2519-40be-a146-56e3f90c791a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba9c6e3-c01f-4be5-8741-3a87772a0661",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_text_splitter = SemanticChunker(embedding_model)\n",
    "\n",
    "semantic_text_splits = semantic_text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a82175b-3062-437e-a9cb-ade726cf64fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_text_splits[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de53a15-c0eb-46c6-8865-56b25f8e0caa",
   "metadata": {},
   "source": [
    "- breakpoint_threshold_type=\"percentile\" tells the chunker to use a dynamic threshold based on similarity percentiles rather than a fixed value. Using breakpoint_threshold_type=\"percentile\" means: “split at points where the similarity is below the Nth percentile of all similarities.” The actual percentile value is controlled by the breakpoint_threshold parameter (default is usually 0.25 or 25% depending on the version). You can define it explicitly like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca06a50-e0f1-40e2-998d-c5089389dcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_text_splitter2 = SemanticChunker(embedding_model,breakpoint_threshold_type=\"percentile\",  breakpoint_threshold_amount=0.175)\n",
    "semantic_text_splits2 = semantic_text_splitter2.split_documents(documents)\n",
    "semantic_text_splits2[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfc025c-a3a3-4e0e-b980-1a92acc51e9c",
   "metadata": {},
   "source": [
    "# Hierarchical chunking\n",
    "Use Case: Complex documents requiring contextual relationships between sections\n",
    "\n",
    "Hierarchical chunking goes a step further by organizing documents into parent and child chunks.\n",
    "\n",
    "By structuring the document hierarchically, the model gains a better understanding of the relationships between different parts of the content, enabling it to provide more contextually relevant and coherent responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f4ef7b-3de9-4d7d-a234-8148f6a0ea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader(input_dir=\"data\")\n",
    "documents = reader.load_data()\n",
    "from llama_index.core.node_parser import HierarchicalNodeParser\n",
    "\n",
    "node_parser = HierarchicalNodeParser.from_defaults(chunk_sizes=[512, 254, 128])\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa51996a-406c-4ee5-a6c0-9bb9dc5d655d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for node in nodes[:2]:\n",
    "    print(len(node.text),node.id_, node.relationships)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
